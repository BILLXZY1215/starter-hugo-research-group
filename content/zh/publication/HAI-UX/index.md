---
title: 'Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization'
authors:
  - Mingming Fan
  - Xianyou Yang
  - Tsz Tung Yu
  - Vera Q. Liao
  - Jian Zhao
author_notes:
  - Corresponding Author
  - 
  -
  -
  -
  -
date: '2022-11-11T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2017-01-01T00:00:00Z'

# Publication type: 
# 0 = Uncategorized; 1 = Accessibility & Aging; 2 = VR/AR/Metaverse; 3 = Human-AI Collaboration; 4 = UX Methodology; 5 = Social Computing; 6 = Sensing;  7 = Thesis; 8 = Patent
publication_types: ['3']

# Publication name and optional abbreviated publication name.
publication: Proceedings of the ACM on Human-Computer Interaction (PACM HCI)
publication_short: CSCW 2022

abstract: 'Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX problems from a usability test session; synchronization refers to the two ways humans and AI collaborate: synchronously and asynchronously. We iteratively designed a tool—AI Assistant—with four versions of UIs corresponding to the two levels of explanations (with/without) and synchronization (sync/async). By adopting a hybrid wizard-of-oz approach to simulating an AI with reasonable performance, we conducted a mixed-method study with 24 UX evaluators identifying UX problems from usability test videos using AI Assistant. Our quantitative and qualitative results show that AI with explanations, regardless of being presented synchronously or asynchronously, provided better support for UX evaluators’ analysis and was perceived more positively; when without explanations, synchronous AI better improved UX evaluators’ performance and engagement compared to the asynchronous AI. Lastly, we present the design implications for AI-assisted UX evaluation and facilitating more effective human-AI collaboration.'

# Summary. An optional shortened abstract.
summary:

keywords: human-AI collaboration, user experience (UX), AI-assisted UX evaluation, explainable AI, intelligent user interface (UI) design, synchronization, explanation, think-aloud usability test

tags:
  - human-AI collaboration
  - user experience (UX)
  - AI-assisted UX evaluation
  - explainable AI
  - intelligent user interface (UI) design
  - synchronization
  - explanation
  - think-aloud usability test
featured: false

links:
  # - name: Custom Link
  #   url: http://example.org
url_pdf: 'https://www.mingmingfan.com/papers/haicollaboration-cscw2022.pdf'
# url_code: 'https://github.com/tjusenchen/Xbot'
# url_dataset: '#'
# url_poster: '#'
# url_project: ''
# url_slides: ''
# url_source: 'https://sites.google.com/view/mobile-accessibility/'
# url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
# projects:
#   - internal-project

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
# slides:
---

<!-- {{< youtube f9lO9tin4tw >}} -->


