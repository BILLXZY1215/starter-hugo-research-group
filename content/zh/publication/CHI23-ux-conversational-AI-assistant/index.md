---
title: 'Collaboration with Conversational AI Assistants for UX Evaluation: Questions and How to Ask them (Voice vs. Text)'
authors:
  - Emily Kuang
  - Ehsan Jahangirzadeh Soure
  - Mingming Fan
  - Jian Zhao
  - Kristen Shinohara
author_notes:
  - 
  - 
  - Corresponding author
  -
  - Corresponding author
date: '2023-02-23T00:00:00Z'
doi: '10.1145/3544548.3581247'

# No need to change the publishDate 
publishDate: '2023-03-01T00:00:00Z'

# Publication type: 
# 0 = Uncategorized; 1 = Accessibility & Aging; 2 = VR/AR/Metaverse; 3 = Human-AI Collaboration; 4 = UX Methodology; 5 = Social Computing; 6 = Sensing;  7 = Thesis; 8 = Patent
publication_types: ['3', '4']

# Publication name and optional abbreviated publication name.
publication: The ACM CHI Conference on Human Factors in Computing Systems 2023
publication_short: CHI 2023

abstract: '近期研究表明人工智能（AI）能够一定程度上帮助用户体验评估师(UX evaluators)更好地分析可用性测试(usability testing)。然而，AI给出的推断往往以一种非交互式的可视化形式呈现给用户体验评估师。评估师可能对AI提供的分析判断有疑惑，但却无法进一步地问询AI。交互式对话助理（interactive conversational assistants）提供了一个动态问询AI的机会，有潜力帮助提高分析效率和评估师的主观能动性。为了了解如何设计一个高效地交互式对话助理，我们进行了一项Wizard of Oz的设计探针（design probe）研究，邀请了20名用户体验分析相关人员通过文本或语音与模拟的人工智能助理进行交互。通过对参与者提出的问题的内容和方式进行分析，我们发现提出的问题大致询问五个类别的信息：用户行为、用户心理模型、人工智能助理提供的帮助、产品和任务信息以及用户人口统计学信息。同时，使用文本人工智能助理的人询问的问题比使用语音人工智能助理的多，但是两者提出的问题的长度相似。文本助理被认为是更加有效率的，但两者在满意度和信任度方面的评价相同。我们还提供了未来用户体验评估的会话式人工智能助理的设计准则与导引。'

# Summary. An optional shortened abstract.
summary: An optional shortened abstract

keywords: User experience, Usability testing, Human-AI collaboration, Conversational assistants

tags:
  - 用户体验
  - 可用性测试
  - 人工智能协作
  - 对话助手
featured: true


url_pdf: 'https://www.mingmingfan.com/papers/CHI23_UX_Design_Probe.pdf'
# url_code: '#'
# url_dataset: '#'
# url_poster: '#'
# url_project: ''
# url_slides: ''
# url_source: '#'
# url_video: '#'


image:
  caption: '文本助理的用户界面，其中包含视频播放器、聊天气泡和聊天线程。'
  focal_point: ''
  preview_only: false


---

<!-- put your youtube/Vimeo video ID here if possible (will update later) -->
<!-- {{< youtube  >}} -->



